

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pytorch_tabnet package &mdash; pytorch_tabnet  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../_static/./default.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="README" href="README.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> pytorch_tabnet
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="README.html">README</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#tabnet-attentive-interpretable-tabular-learning">TabNet : Attentive Interpretable Tabular Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#what-problems-does-pytorch-tabnet-handles">What problems does pytorch-tabnet handles?</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#how-to-use-it">How to use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#useful-links">Useful links</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">pytorch_tabnet package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.metrics">pytorch_tabnet.metrics module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.sparsemax">pytorch_tabnet.sparsemax module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.callbacks">pytorch_tabnet.callbacks module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.tab_network">pytorch_tabnet.tab_network module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.utils">pytorch_tabnet.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.multiclass_utils">pytorch_tabnet.multiclass_utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multi-class-multi-label-utility-function">Multi-class / multi-label utility function</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.abstract_model">pytorch_tabnet.abstract_model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.multitask">pytorch_tabnet.multitask module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.tab_model">pytorch_tabnet.tab_model module</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pytorch_tabnet</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>pytorch_tabnet package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/generated_docs/pytorch_tabnet.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pytorch-tabnet-package">
<h1>pytorch_tabnet package<a class="headerlink" href="#pytorch-tabnet-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-pytorch_tabnet.metrics">
<span id="pytorch-tabnet-metrics-module"></span><h2>pytorch_tabnet.metrics module<a class="headerlink" href="#module-pytorch_tabnet.metrics" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.metrics.AUC">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">AUC</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#AUC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.AUC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>AUC.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.Accuracy">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">Accuracy</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#Accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.Accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Accuracy.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.BalancedAccuracy">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">BalancedAccuracy</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#BalancedAccuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.BalancedAccuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Balanced Accuracy.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.LogLoss">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">LogLoss</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#LogLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.LogLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>LogLoss.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.MAE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">MAE</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#MAE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.MAE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Mean Absolute Error.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.MSE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">MSE</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#MSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.MSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Mean Squared Error.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.Metric">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">Metric</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#Metric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.Metric" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.metrics.Metric.get_metrics_by_names">
<em class="property">classmethod </em><code class="sig-name descname">get_metrics_by_names</code><span class="sig-paren">(</span><em class="sig-param">names</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#Metric.get_metrics_by_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.Metric.get_metrics_by_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Get list of metric classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cls</strong> (<a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><em>Metric</em></a>) – Metric class.</p></li>
<li><p><strong>names</strong> (<em>list</em>) – List of metric names.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>metrics</strong> – List of metric classes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.MetricContainer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">MetricContainer</code><span class="sig-paren">(</span><em class="sig-param">metric_names: List[str], prefix: str = ''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#MetricContainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.MetricContainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Container holding a list of metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric_names</strong> (<em>list of str</em>) – List of metric names.</p></li>
<li><p><strong>prefix</strong> (<em>str</em>) – Prefix of metric names.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytorch_tabnet.metrics.MetricContainer.metric_names">
<code class="sig-name descname">metric_names</code><em class="property">: List[str]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.metrics.MetricContainer.metric_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.metrics.MetricContainer.prefix">
<code class="sig-name descname">prefix</code><em class="property">: str</em><em class="property"> = ''</em><a class="headerlink" href="#pytorch_tabnet.metrics.MetricContainer.prefix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.RMSE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">RMSE</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#RMSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.RMSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Root Mean Squared Error.</p>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.metrics.check_metrics">
<code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">check_metrics</code><span class="sig-paren">(</span><em class="sig-param">metrics</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#check_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.check_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if custom metrics are provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>metrics</strong> (<em>list of str</em><em> or </em><em>classes</em>) – List with built-in metrics (str) or custom metrics (classes).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>val_metrics</strong> – List of metric names.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pytorch_tabnet.sparsemax">
<span id="pytorch-tabnet-sparsemax-module"></span><h2>pytorch_tabnet.sparsemax module<a class="headerlink" href="#module-pytorch_tabnet.sparsemax" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.sparsemax.Entmax15">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">Entmax15</code><span class="sig-paren">(</span><em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmax15.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.sparsemax.Entmax15Function">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">Entmax15Function</code><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15Function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15Function" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>An implementation of exact Entmax with alpha=1.5 (B. Peters, V. Niculae, A. Martins). See
:cite:<a href="#id1"><span class="problematic" id="id2">`</span></a><a class="reference external" href="https://arxiv.org/abs/1905.05702">https://arxiv.org/abs/1905.05702</a> for detailed description.
Source: <a class="reference external" href="https://github.com/deep-spin/entmax">https://github.com/deep-spin/entmax</a></p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmax15Function.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15Function.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15Function.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs did <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmax15Function.forward" title="pytorch_tabnet.sparsemax.Entmax15Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> return, and it should return as many
tensors, as there were inputs to <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmax15Function.forward" title="pytorch_tabnet.sparsemax.Entmax15Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the
gradient w.r.t the given output, and each returned value should be the
gradient w.r.t. the corresponding input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmax15Function.backward" title="pytorch_tabnet.sparsemax.Entmax15Function.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmax15Function.forward" title="pytorch_tabnet.sparsemax.Entmax15Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmax15Function.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">input</em>, <em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15Function.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15Function.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.sparsemax.Entmoid15">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">Entmoid15</code><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmoid15"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmoid15" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>A highly optimized equivalent of labda x: Entmax15([x, 0])</p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmoid15.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmoid15.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmoid15.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs did <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmoid15.forward" title="pytorch_tabnet.sparsemax.Entmoid15.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> return, and it should return as many
tensors, as there were inputs to <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmoid15.forward" title="pytorch_tabnet.sparsemax.Entmoid15.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the
gradient w.r.t the given output, and each returned value should be the
gradient w.r.t. the corresponding input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmoid15.backward" title="pytorch_tabnet.sparsemax.Entmoid15.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmoid15.forward" title="pytorch_tabnet.sparsemax.Entmoid15.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmoid15.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmoid15.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmoid15.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.sparsemax.Sparsemax">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">Sparsemax</code><span class="sig-paren">(</span><em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Sparsemax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Sparsemax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Sparsemax.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Sparsemax.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Sparsemax.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.sparsemax.SparsemaxFunction">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">SparsemaxFunction</code><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#SparsemaxFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.SparsemaxFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>An implementation of sparsemax (Martins &amp; Astudillo, 2016). See
<a href="#id3"><span class="problematic" id="id4">:cite:`DBLP:journals/corr/MartinsA16`</span></a> for detailed description.
By Ben Peters and Vlad Niculae</p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.SparsemaxFunction.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#SparsemaxFunction.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs did <a class="reference internal" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.forward" title="pytorch_tabnet.sparsemax.SparsemaxFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> return, and it should return as many
tensors, as there were inputs to <a class="reference internal" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.forward" title="pytorch_tabnet.sparsemax.SparsemaxFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the
gradient w.r.t the given output, and each returned value should be the
gradient w.r.t. the corresponding input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.backward" title="pytorch_tabnet.sparsemax.SparsemaxFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.forward" title="pytorch_tabnet.sparsemax.SparsemaxFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.sparsemax.SparsemaxFunction.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">input</em>, <em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#SparsemaxFunction.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>sparsemax: normalizing sparse transform (a la softmax)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<em>torch.autograd.function._ContextMethodMixin</em>) – </p></li>
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – any shape</p></li>
<li><p><strong>dim</strong> (<em>int</em>) – dimension along which to apply sparsemax</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>output</strong> – same shape as input</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.sparsemax.entmax15">
<code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">entmax15</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pytorch_tabnet.sparsemax.entmax15" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.sparsemax.entmoid15">
<code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">entmoid15</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pytorch_tabnet.sparsemax.entmoid15" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.sparsemax.sparsemax">
<code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">sparsemax</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pytorch_tabnet.sparsemax.sparsemax" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-pytorch_tabnet.callbacks">
<span id="pytorch-tabnet-callbacks-module"></span><h2>pytorch_tabnet.callbacks module<a class="headerlink" href="#module-pytorch_tabnet.callbacks" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.callbacks.Callback">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">Callback</code><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Abstract base class used to build new callbacks.</p>
<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_batch_begin">
<code class="sig-name descname">on_batch_begin</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_batch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_batch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_batch_end">
<code class="sig-name descname">on_batch_end</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_epoch_begin">
<code class="sig-name descname">on_epoch_begin</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_epoch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_epoch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_train_begin">
<code class="sig-name descname">on_train_begin</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_train_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_train_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_train_end">
<code class="sig-name descname">on_train_end</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_train_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">params</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.set_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.set_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.set_trainer">
<code class="sig-name descname">set_trainer</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.set_trainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.set_trainer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.callbacks.CallbackContainer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">CallbackContainer</code><span class="sig-paren">(</span><em class="sig-param">callbacks: List[pytorch_tabnet.callbacks.Callback] = &lt;factory&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Container holding a list of callbacks.</p>
<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.append">
<code class="sig-name descname">append</code><span class="sig-paren">(</span><em class="sig-param">callback</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.append" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.callbacks">
<code class="sig-name descname">callbacks</code><em class="property">: List[Callback]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.callbacks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_batch_begin">
<code class="sig-name descname">on_batch_begin</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_batch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_batch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_batch_end">
<code class="sig-name descname">on_batch_end</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_epoch_begin">
<code class="sig-name descname">on_epoch_begin</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_epoch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_epoch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_train_begin">
<code class="sig-name descname">on_train_begin</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_train_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_train_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_train_end">
<code class="sig-name descname">on_train_end</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_train_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">params</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.set_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.set_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.set_trainer">
<code class="sig-name descname">set_trainer</code><span class="sig-paren">(</span><em class="sig-param">trainer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.set_trainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.set_trainer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.callbacks.EarlyStopping">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">EarlyStopping</code><span class="sig-paren">(</span><em class="sig-param">early_stopping_metric: str</em>, <em class="sig-param">is_maximize: bool</em>, <em class="sig-param">tol: float = 0.0</em>, <em class="sig-param">patience: int = 5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#EarlyStopping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.callbacks.Callback" title="pytorch_tabnet.callbacks.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.callbacks.Callback</span></code></a></p>
<p>EarlyStopping callback to exit the training loop if early_stopping_metric
does not improve by a certain amount for a certain
number of epochs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>early_stopping_metric</strong> (<em>str</em>) – Early stopping metric name</p></li>
<li><p><strong>is_maximize</strong> (<em>bool</em>) – Whether to maximize or not early_stopping_metric</p></li>
<li><p><strong>tol</strong> (<em>float</em>) – minimum change in monitored value to qualify as improvement.
This number should be positive.</p></li>
<li><p><strong>patience</strong> (<em>integer</em>) – number of epochs to wait for improvment before terminating.
the counter be reset after each improvment</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.early_stopping_metric">
<code class="sig-name descname">early_stopping_metric</code><em class="property">: str</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.early_stopping_metric" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.is_maximize">
<code class="sig-name descname">is_maximize</code><em class="property">: bool</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.is_maximize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#EarlyStopping.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.on_train_end">
<code class="sig-name descname">on_train_end</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#EarlyStopping.on_train_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.patience">
<code class="sig-name descname">patience</code><em class="property">: int</em><em class="property"> = 5</em><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.patience" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.tol">
<code class="sig-name descname">tol</code><em class="property">: float</em><em class="property"> = 0.0</em><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.tol" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.callbacks.History">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">History</code><span class="sig-paren">(</span><em class="sig-param">trainer: Any</em>, <em class="sig-param">verbose: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.callbacks.Callback" title="pytorch_tabnet.callbacks.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.callbacks.Callback</span></code></a></p>
<p>Callback that records events into a <cite>History</cite> object.
This callback is automatically applied to
every SuperModule.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainer</strong> (<em>DeepRecoModel</em>) – Model class to train</p></li>
<li><p><strong>verbose</strong> (<em>int</em>) – Print results every verbose iteration</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pytorch_tabnet.callbacks.History.on_batch_end">
<code class="sig-name descname">on_batch_end</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History.on_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History.on_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.History.on_epoch_begin">
<code class="sig-name descname">on_epoch_begin</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History.on_epoch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History.on_epoch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.History.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.History.on_train_begin">
<code class="sig-name descname">on_train_begin</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History.on_train_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History.on_train_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.History.trainer">
<code class="sig-name descname">trainer</code><em class="property">: Any</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.History.trainer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.History.verbose">
<code class="sig-name descname">verbose</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.callbacks.History.verbose" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">LRSchedulerCallback</code><span class="sig-paren">(</span><em class="sig-param">scheduler_fn: Any</em>, <em class="sig-param">optimizer: Any</em>, <em class="sig-param">scheduler_params: dict</em>, <em class="sig-param">early_stopping_metric: str</em>, <em class="sig-param">is_batch_level: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#LRSchedulerCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.callbacks.Callback" title="pytorch_tabnet.callbacks.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.callbacks.Callback</span></code></a></p>
<p>Wrapper for most torch scheduler functions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scheduler_fn</strong> (<em>torch.optim.lr_scheduler</em>) – Torch scheduling class</p></li>
<li><p><strong>scheduler_params</strong> (<em>dict</em>) – Dictionnary containing all parameters for the scheduler_fn</p></li>
<li><p><strong>is_batch_level</strong> (<em>bool</em><em> (</em><em>default = False</em><em>)</em>) – If set to False : lr updates will happen at every epoch
If set to True : lr updates happen at every batch
Set this to True for OneCycleLR for example</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.early_stopping_metric">
<code class="sig-name descname">early_stopping_metric</code><em class="property">: str</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.early_stopping_metric" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.is_batch_level">
<code class="sig-name descname">is_batch_level</code><em class="property">: bool</em><em class="property"> = False</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.is_batch_level" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.on_batch_end">
<code class="sig-name descname">on_batch_end</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#LRSchedulerCallback.on_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.on_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#LRSchedulerCallback.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.optimizer">
<code class="sig-name descname">optimizer</code><em class="property">: Any</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.optimizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.scheduler_fn">
<code class="sig-name descname">scheduler_fn</code><em class="property">: Any</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.scheduler_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property">: dict</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pytorch_tabnet.tab_network">
<span id="pytorch-tabnet-tab-network-module"></span><h2>pytorch_tabnet.tab_network module<a class="headerlink" href="#module-pytorch_tabnet.tab_network" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.tab_network.AttentiveTransformer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">AttentiveTransformer</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em>, <em class="sig-param">mask_type='sparsemax'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#AttentiveTransformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.AttentiveTransformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.AttentiveTransformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">priors</em>, <em class="sig-param">processed_feat</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#AttentiveTransformer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.AttentiveTransformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.EmbeddingGenerator">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">EmbeddingGenerator</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">cat_dims</em>, <em class="sig-param">cat_idxs</em>, <em class="sig-param">cat_emb_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#EmbeddingGenerator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.EmbeddingGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Classical embeddings generator</p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.EmbeddingGenerator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#EmbeddingGenerator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.EmbeddingGenerator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply embdeddings to inputs
Inputs should be (batch_size, input_dim)
Outputs will be of size (batch_size, self.post_embed_dim)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.FeatTransformer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">FeatTransformer</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">shared_layers</em>, <em class="sig-param">n_glu_independent</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#FeatTransformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.FeatTransformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.FeatTransformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#FeatTransformer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.FeatTransformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.GBN">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">GBN</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GBN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GBN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Ghost Batch Normalization
<a class="reference external" href="https://arxiv.org/abs/1705.08741">https://arxiv.org/abs/1705.08741</a></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.GBN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GBN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GBN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.GLU_Block">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">GLU_Block</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">n_glu=2</em>, <em class="sig-param">first=False</em>, <em class="sig-param">shared_layers=None</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GLU_Block"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Block" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Independant GLU block, specific to each step</p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.GLU_Block.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GLU_Block.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Block.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.GLU_Layer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">GLU_Layer</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">fc=None</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GLU_Layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.GLU_Layer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GLU_Layer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Layer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.TabNet">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">TabNet</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">n_d=8</em>, <em class="sig-param">n_a=8</em>, <em class="sig-param">n_steps=3</em>, <em class="sig-param">gamma=1.3</em>, <em class="sig-param">cat_idxs=[]</em>, <em class="sig-param">cat_dims=[]</em>, <em class="sig-param">cat_emb_dim=1</em>, <em class="sig-param">n_independent=2</em>, <em class="sig-param">n_shared=2</em>, <em class="sig-param">epsilon=1e-15</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em>, <em class="sig-param">device_name='auto'</em>, <em class="sig-param">mask_type='sparsemax'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNet.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNet.forward_masks">
<code class="sig-name descname">forward_masks</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNet.forward_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNet.forward_masks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.TabNetNoEmbeddings">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">TabNetNoEmbeddings</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">n_d=8</em>, <em class="sig-param">n_a=8</em>, <em class="sig-param">n_steps=3</em>, <em class="sig-param">gamma=1.3</em>, <em class="sig-param">n_independent=2</em>, <em class="sig-param">n_shared=2</em>, <em class="sig-param">epsilon=1e-15</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em>, <em class="sig-param">mask_type='sparsemax'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetNoEmbeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetNoEmbeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetNoEmbeddings.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetNoEmbeddings.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetNoEmbeddings.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetNoEmbeddings.forward_masks">
<code class="sig-name descname">forward_masks</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetNoEmbeddings.forward_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetNoEmbeddings.forward_masks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.tab_network.initialize_glu">
<code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">initialize_glu</code><span class="sig-paren">(</span><em class="sig-param">module</em>, <em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#initialize_glu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.initialize_glu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.tab_network.initialize_non_glu">
<code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">initialize_non_glu</code><span class="sig-paren">(</span><em class="sig-param">module</em>, <em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#initialize_non_glu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.initialize_non_glu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-pytorch_tabnet.utils">
<span id="pytorch-tabnet-utils-module"></span><h2>pytorch_tabnet.utils module<a class="headerlink" href="#module-pytorch_tabnet.utils" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.utils.PredictDataset">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">PredictDataset</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#PredictDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.PredictDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>Format for numpy array</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>2D array</em>) – The input matrix</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.utils.TorchDataset">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">TorchDataset</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#TorchDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.TorchDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>Format for numpy array</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>2D array</em>) – The input matrix</p></li>
<li><p><strong>y</strong> (<em>2D array</em>) – The one-hot encoded target</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.create_dataloaders">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">create_dataloaders</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">num_workers</em>, <em class="sig-param">drop_last</em>, <em class="sig-param">pin_memory</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#create_dataloaders"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.create_dataloaders" title="Permalink to this definition">¶</a></dt>
<dd><p>Create dataloaders with or wihtout subsampling depending on weights and balanced.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Training data</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Mapped Training targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y)</p></li>
<li><p><strong>weights</strong> (<em>either 0</em><em>, </em><em>1</em><em>, </em><em>dict</em><em> or </em><em>iterable</em>) – <p>if 0 (default) : no weights will be applied
if 1 : classification only, will balanced class with inverse frequency
if dict : keys are corresponding class values are sample weights
if iterable : list or np array must be of length equal to nb elements</p>
<blockquote>
<div><p>in the training set</p>
</div></blockquote>
</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many samples per batch to load</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – how many subprocesses to use for data loading. 0 means that the data
will be loaded in the main process</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – set to True to drop the last incomplete batch, if the dataset size is not
divisible by the batch size. If False and the size of dataset is not
divisible by the batch size, then the last batch will be smaller</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to pin GPU memory during training</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>train_dataloader, valid_dataloader</strong> – Training and validation dataloaders</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.DataLoader, torch.DataLoader</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.create_explain_matrix">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">create_explain_matrix</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">cat_emb_dim</em>, <em class="sig-param">cat_idxs</em>, <em class="sig-param">post_embed_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#create_explain_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.create_explain_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a computational trick.
In order to rapidly sum importances from same embeddings
to the initial index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – Initial input dim</p></li>
<li><p><strong>cat_emb_dim</strong> (<em>int</em><em> or </em><em>list of int</em>) – if int : size of embedding for all categorical feature
if list of int : size of embedding for each categorical feature</p></li>
<li><p><strong>cat_idxs</strong> (<em>list of int</em>) – Initial position of categorical features</p></li>
<li><p><strong>post_embed_dim</strong> (<em>int</em>) – Post embedding inputs dimension</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>reducing_matrix</strong> – Matrix of dim (post_embed_dim, input_dim)  to performe reduce</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.define_device">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">define_device</code><span class="sig-paren">(</span><em class="sig-param">device_name</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#define_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.define_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the device to use during training and inference.
If auto it will detect automatically whether to use cuda or cpu</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device_name</strong> (<em>str</em>) – Either “auto”, “cpu” or “cuda”</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Either “cpu” or “cuda”</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.filter_weights">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">filter_weights</code><span class="sig-paren">(</span><em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#filter_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.filter_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>This function makes sure that weights are in correct format for
regression and multitask TabNet</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weights</strong> (<em>int</em><em>, </em><em>dict</em><em> or </em><em>list</em>) – Initial weights parameters given by user</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>None</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>This function will only throw an error if format is wrong</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.validate_eval_set">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">validate_eval_set</code><span class="sig-paren">(</span><em class="sig-param">eval_set</em>, <em class="sig-param">eval_name</em>, <em class="sig-param">X_train</em>, <em class="sig-param">y_train</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#validate_eval_set"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.validate_eval_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if the shapes of eval_set are compatible with (X_train, y_train).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).
The last one is used for early stopping</p></li>
<li><p><strong>eval_name</strong> (<em>list of str</em>) – List of eval set names.</p></li>
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train owned products</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targeted products</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>eval_names</strong> (<em>list of str</em>) – Validated list of eval_names.</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – Validated list of eval_set.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pytorch_tabnet.multiclass_utils">
<span id="pytorch-tabnet-multiclass-utils-module"></span><h2>pytorch_tabnet.multiclass_utils module<a class="headerlink" href="#module-pytorch_tabnet.multiclass_utils" title="Permalink to this headline">¶</a></h2>
<div class="section" id="multi-class-multi-label-utility-function">
<h3>Multi-class / multi-label utility function<a class="headerlink" href="#multi-class-multi-label-utility-function" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.assert_all_finite">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">assert_all_finite</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">allow_nan=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#assert_all_finite"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.assert_all_finite" title="Permalink to this definition">¶</a></dt>
<dd><p>Throw a ValueError if X contains NaN or infinity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array</em><em> or </em><em>sparse matrix</em>) – </p></li>
<li><p><strong>allow_nan</strong> (<em>bool</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.check_classification_targets">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">check_classification_targets</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#check_classification_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.check_classification_targets" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure that target y is of a non-regression type.</p>
<dl class="simple">
<dt>Only the following target types (as defined in type_of_target) are allowed:</dt><dd><p>‘binary’, ‘multiclass’, ‘multiclass-multioutput’,
‘multilabel-indicator’, ‘multilabel-sequences’</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (<em>array-like</em>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.check_output_dim">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">check_output_dim</code><span class="sig-paren">(</span><em class="sig-param">labels</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#check_output_dim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.check_output_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.infer_multitask_output">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">infer_multitask_output</code><span class="sig-paren">(</span><em class="sig-param">y_train</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#infer_multitask_output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.infer_multitask_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Infer output_dim from targets
This is for multiple tasks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y_train</strong> (<em>np.ndarray</em>) – Training targets</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>tasks_dims</strong> (<em>list</em>) – Number of classes for output</p></li>
<li><p><strong>tasks_labels</strong> (<em>list</em>) – List of sorted list of initial classes</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.infer_output_dim">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">infer_output_dim</code><span class="sig-paren">(</span><em class="sig-param">y_train</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#infer_output_dim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.infer_output_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Infer output_dim from targets</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y_train</strong> (<em>np.array</em>) – Training targets</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>output_dim</strong> (<em>int</em>) – Number of classes for output</p></li>
<li><p><strong>train_labels</strong> (<em>list</em>) – Sorted list of initial classes</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.is_multilabel">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">is_multilabel</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#is_multilabel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.is_multilabel" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if <code class="docutils literal notranslate"><span class="pre">y</span></code> is in a multilabel format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>]</em>) – Target values.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – Return <code class="docutils literal notranslate"><span class="pre">True</span></code>, if <code class="docutils literal notranslate"><span class="pre">y</span></code> is in a multilabel format, else <code class="docutils literal notranslate"><span class="pre">`False</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.utils.multiclass</span> <span class="kn">import</span> <span class="n">is_multilabel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[]])</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]))</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.type_of_target">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">type_of_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#type_of_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.type_of_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Determine the type of data indicated by the target.</p>
<p>Note that this type is the most specific type that can be inferred.
For example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">binary</span></code> is more specific but compatible with <code class="docutils literal notranslate"><span class="pre">multiclass</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">multiclass</span></code> of integers is more specific but compatible with
<code class="docutils literal notranslate"><span class="pre">continuous</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">multilabel-indicator</span></code> is more specific but compatible with
<code class="docutils literal notranslate"><span class="pre">multiclass-multioutput</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (<em>array-like</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>target_type</strong> – One of:</p>
<ul class="simple">
<li><p>’continuous’: <cite>y</cite> is an array-like of floats that are not all
integers, and is 1d or a column vector.</p></li>
<li><p>’continuous-multioutput’: <cite>y</cite> is a 2d array of floats that are
not all integers, and both dimensions are of size &gt; 1.</p></li>
<li><p>’binary’: <cite>y</cite> contains &lt;= 2 discrete values and is 1d or a column
vector.</p></li>
<li><p>’multiclass’: <cite>y</cite> contains more than two discrete values, is not a
sequence of sequences, and is 1d or a column vector.</p></li>
<li><p>’multiclass-multioutput’: <cite>y</cite> is a 2d array that contains more
than two discrete values, is not a sequence of sequences, and both
dimensions are of size &gt; 1.</p></li>
<li><p>’multilabel-indicator’: <cite>y</cite> is a label indicator matrix, an array
of two dimensions with at least two columns, and at most 2 unique
values.</p></li>
<li><p>’unknown’: <cite>y</cite> is array-like but none of the above, such as a 3d
array, sequence of sequences, or an array of non-sequence objects.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">])</span>
<span class="go">&#39;continuous&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">&#39;binary&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">])</span>
<span class="go">&#39;binary&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="go">&#39;binary&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">&#39;multiclass&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="go">&#39;multiclass&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="go">&#39;multiclass&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]))</span>
<span class="go">&#39;multiclass-multioutput&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="go">&#39;multiclass-multioutput&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">]]))</span>
<span class="go">&#39;continuous-multioutput&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]))</span>
<span class="go">&#39;multilabel-indicator&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.unique_labels">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">unique_labels</code><span class="sig-paren">(</span><em class="sig-param">*ys</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#unique_labels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.unique_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract an ordered array of unique labels</p>
<dl class="simple">
<dt>We don’t allow:</dt><dd><ul class="simple">
<li><p>mix of multilabel and multiclass (single label) targets</p></li>
<li><p>mix of label indicator matrix and anything else,
because there are no explicit labels)</p></li>
<li><p>mix of label indicator matrices of different sizes</p></li>
<li><p>mix of string and integer labels</p></li>
</ul>
</dd>
</dl>
<p>At the moment, we also don’t allow “multiclass-multioutput” input type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*ys</strong> (<em>array-likes</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – An ordered array of unique labels.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape [n_unique_labels]</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.utils.multiclass</span> <span class="kn">import</span> <span class="n">unique_labels</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unique_labels</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="go">array([3, 5, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unique_labels</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="go">array([1, 2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unique_labels</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>
<span class="go">array([ 1,  2,  5, 10, 11])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="module-pytorch_tabnet.abstract_model">
<span id="pytorch-tabnet-abstract-model-module"></span><h2>pytorch_tabnet.abstract_model module<a class="headerlink" href="#module-pytorch_tabnet.abstract_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.abstract_model.TabModel">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.abstract_model.</code><code class="sig-name descname">TabModel</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Class for TabNet model.</p>
<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property">: List[int]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.cat_emb_dim">
<code class="sig-name descname">cat_emb_dim</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.cat_emb_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property">: List[int]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.clip_value">
<code class="sig-name descname">clip_value</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.clip_value" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.compute_loss">
<em class="property">abstract </em><code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">y_score</em>, <em class="sig-param">y_true</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_score</strong> (a :tensor: <cite>torch.Tensor</cite>) – Score matrix</p></li>
<li><p><strong>y_true</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.device_name">
<code class="sig-name descname">device_name</code><em class="property">: str</em><em class="property"> = 'auto'</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.device_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.epsilon">
<code class="sig-name descname">epsilon</code><em class="property">: float</em><em class="property"> = 1e-15</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.epsilon" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.explain">
<code class="sig-name descname">explain</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.explain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.explain" title="Permalink to this definition">¶</a></dt>
<dd><p>Return local explanation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (tensor: <cite>torch.Tensor</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>M_explain</strong> (<em>matrix</em>) – Importance per sample, per columns.</p></li>
<li><p><strong>masks</strong> (<em>matrix</em>) – Sparse matrix showing attention masks used by network.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set=None</em>, <em class="sig-param">eval_name=None</em>, <em class="sig-param">eval_metric=None</em>, <em class="sig-param">loss_fn=None</em>, <em class="sig-param">weights=0</em>, <em class="sig-param">max_epochs=100</em>, <em class="sig-param">patience=10</em>, <em class="sig-param">batch_size=1024</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">num_workers=0</em>, <em class="sig-param">drop_last=False</em>, <em class="sig-param">callbacks=None</em>, <em class="sig-param">pin_memory=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train a neural network stored in self.network
Using train_dataloader for training data and
valid_dataloader for validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).
The last one is used for early stopping</p></li>
<li><p><strong>eval_name</strong> (<em>list of str</em>) – List of eval set names.</p></li>
<li><p><strong>eval_metric</strong> (<em>list of str</em>) – List of evaluation metrics.
The last metric is used for early stopping.</p></li>
<li><p><strong>loss_fn</strong> (<em>callable</em><em> or </em><em>None</em>) – a PyTorch loss function</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing
dict for custom weights per class</p></li>
<li><p><strong>max_epochs</strong> (<em>int</em>) – Maximum number of epochs during training</p></li>
<li><p><strong>patience</strong> (<em>int</em>) – Number of consecutive non improving epoch before early stopping</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Training batch size</p></li>
<li><p><strong>virtual_batch_size</strong> (<em>int</em>) – Batch size for Ghost Batch Normalization (virtual_batch_size &lt; batch_size)</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers used in torch.utils.data.DataLoader</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – Whether to drop last batch during training</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback function</em>) – List of custom callbacks</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to set pin_memory to True or False during training</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.gamma">
<code class="sig-name descname">gamma</code><em class="property">: float</em><em class="property"> = 1.3</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.gamma" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.input_dim">
<code class="sig-name descname">input_dim</code><em class="property">: int</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.input_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.lambda_sparse">
<code class="sig-name descname">lambda_sparse</code><em class="property">: float</em><em class="property"> = 0.001</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.lambda_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">filepath</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.load_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load TabNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filepath</strong> (<em>str</em>) – Path of the model.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.mask_type">
<code class="sig-name descname">mask_type</code><em class="property">: str</em><em class="property"> = 'sparsemax'</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.mask_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.momentum">
<code class="sig-name descname">momentum</code><em class="property">: float</em><em class="property"> = 0.02</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.momentum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_a">
<code class="sig-name descname">n_a</code><em class="property">: int</em><em class="property"> = 8</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_a" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_d">
<code class="sig-name descname">n_d</code><em class="property">: int</em><em class="property"> = 8</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_independent">
<code class="sig-name descname">n_independent</code><em class="property">: int</em><em class="property"> = 2</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_independent" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_shared">
<code class="sig-name descname">n_shared</code><em class="property">: int</em><em class="property"> = 2</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_steps">
<code class="sig-name descname">n_steps</code><em class="property">: int</em><em class="property"> = 3</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.optimizer_fn">
<code class="sig-name descname">optimizer_fn</code><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.optimizer_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.adam.Adam</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property">: Dict</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.output_dim">
<code class="sig-name descname">output_dim</code><em class="property">: int</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.output_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>predictions</strong> – Predictions of the regression problem</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.prepare_target">
<em class="property">abstract </em><code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.save_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saving TabNet model in two distinct files.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>str</em>) – Path of the model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>input filepath with “.zip” appended</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.scheduler_fn">
<code class="sig-name descname">scheduler_fn</code><em class="property">: Any</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.scheduler_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property">: Dict</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.seed">
<code class="sig-name descname">seed</code><em class="property">: int</em><em class="property"> = 0</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.update_fit_params">
<em class="property">abstract </em><code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.verbose">
<code class="sig-name descname">verbose</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.verbose" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pytorch_tabnet.multitask">
<span id="pytorch-tabnet-multitask-module"></span><h2>pytorch_tabnet.multitask module<a class="headerlink" href="#module-pytorch_tabnet.multitask" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.multitask.</code><code class="sig-name descname">TabNetMultiTaskClassifier</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.abstract_model.TabModel" title="pytorch_tabnet.abstract_model.TabModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.abstract_model.TabModel</span></code></a></p>
<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">y_true</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the loss according to network output and targets</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_pred</strong> (<em>list of tensors</em>) – Output of network</p></li>
<li><p><strong>y_true</strong> (<em>LongTensor</em>) – Targets label encoded</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – output of loss function(s)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>results</strong> – Predictions of the most probable class</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions for classification on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>res</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.prepare_target">
<code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.stack_batches">
<code class="sig-name descname">stack_batches</code><span class="sig-paren">(</span><em class="sig-param">list_y_true</em>, <em class="sig-param">list_y_score</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.stack_batches"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.stack_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.update_fit_params">
<code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pytorch_tabnet.tab_model">
<span id="pytorch-tabnet-tab-model-module"></span><h2>pytorch_tabnet.tab_model module<a class="headerlink" href="#module-pytorch_tabnet.tab_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_model.</code><code class="sig-name descname">TabNetClassifier</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.abstract_model.TabModel" title="pytorch_tabnet.abstract_model.TabModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.abstract_model.TabModel</span></code></a></p>
<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">y_true</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_score</strong> (a :tensor: <cite>torch.Tensor</cite>) – Score matrix</p></li>
<li><p><strong>y_true</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.predict_func">
<code class="sig-name descname">predict_func</code><span class="sig-paren">(</span><em class="sig-param">outputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.predict_func"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.predict_func" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions for classification on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>res</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.prepare_target">
<code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.stack_batches">
<code class="sig-name descname">stack_batches</code><span class="sig-paren">(</span><em class="sig-param">list_y_true</em>, <em class="sig-param">list_y_score</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.stack_batches"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.stack_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.update_fit_params">
<code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.weight_updater">
<code class="sig-name descname">weight_updater</code><span class="sig-paren">(</span><em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.weight_updater"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.weight_updater" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates weights dictionnary according to target_mapper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dict</em>) – Given weights for balancing training.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Same bool if weights are bool, updated dict otherwise.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bool or dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_model.</code><code class="sig-name descname">TabNetRegressor</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.abstract_model.TabModel" title="pytorch_tabnet.abstract_model.TabModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.abstract_model.TabModel</span></code></a></p>
<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">y_true</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_score</strong> (a :tensor: <cite>torch.Tensor</cite>) – Score matrix</p></li>
<li><p><strong>y_true</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.predict_func">
<code class="sig-name descname">predict_func</code><span class="sig-paren">(</span><em class="sig-param">outputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.predict_func"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.predict_func" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.prepare_target">
<code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.stack_batches">
<code class="sig-name descname">stack_batches</code><span class="sig-paren">(</span><em class="sig-param">list_y_true</em>, <em class="sig-param">list_y_score</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.stack_batches"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.stack_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.update_fit_params">
<code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="README.html" class="btn btn-neutral float-left" title="README" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019, Dreamquark

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>