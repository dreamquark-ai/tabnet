

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pytorch_tabnet package &mdash; pytorch_tabnet  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../_static/./default.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="README" href="README.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> pytorch_tabnet
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="README.html">README</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#tabnet-attentive-interpretable-tabular-learning">TabNet : Attentive Interpretable Tabular Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#what-is-new">What is new ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#what-problems-does-pytorch-tabnet-handle">What problems does pytorch-tabnet handle?</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#how-to-use-it">How to use it?</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#semi-supervised-pre-training">Semi-supervised pre-training</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#data-augmentation-on-the-fly">Data augmentation on the fly</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#easy-saving-and-loading">Easy saving and loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#useful-links">Useful links</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">pytorch_tabnet package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.pretraining_utils">pytorch_tabnet.pretraining_utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.augmentations">pytorch_tabnet.augmentations module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.tab_network">pytorch_tabnet.tab_network module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.metrics">pytorch_tabnet.metrics module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.tab_model">pytorch_tabnet.tab_model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.sparsemax">pytorch_tabnet.sparsemax module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.callbacks">pytorch_tabnet.callbacks module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.abstract_model">pytorch_tabnet.abstract_model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.pretraining">pytorch_tabnet.pretraining module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.utils">pytorch_tabnet.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.multitask">pytorch_tabnet.multitask module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-pytorch_tabnet.multiclass_utils">pytorch_tabnet.multiclass_utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multi-class-multi-label-utility-function">Multi-class / multi-label utility function</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pytorch_tabnet</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>pytorch_tabnet package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/generated_docs/pytorch_tabnet.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="pytorch-tabnet-package">
<h1>pytorch_tabnet package<a class="headerlink" href="#pytorch-tabnet-package" title="Permalink to this headline">¶</a></h1>
<section id="module-pytorch_tabnet.pretraining_utils">
<span id="pytorch-tabnet-pretraining-utils-module"></span><h2>pytorch_tabnet.pretraining_utils module<a class="headerlink" href="#module-pytorch_tabnet.pretraining_utils" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="pytorch_tabnet.pretraining_utils.create_dataloaders">
<code class="sig-prename descclassname">pytorch_tabnet.pretraining_utils.</code><code class="sig-name descname">create_dataloaders</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">num_workers</em>, <em class="sig-param">drop_last</em>, <em class="sig-param">pin_memory</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining_utils.html#create_dataloaders"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining_utils.create_dataloaders" title="Permalink to this definition">¶</a></dt>
<dd><p>Create dataloaders with or without subsampling depending on weights and balanced.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em><em> or </em><em>scipy.sparse.csr_matrix</em>) – Training data</p></li>
<li><p><strong>eval_set</strong> (<em>list of np.array</em><em> (</em><em>for Xs and ys</em><em>) or </em><em>scipy.sparse.csr_matrix</em><em> (</em><em>for Xs</em><em>)</em>) – List of eval sets</p></li>
<li><p><strong>weights</strong> (<em>either 0</em><em>, </em><em>1</em><em>, </em><em>dict</em><em> or </em><em>iterable</em>) – <p>if 0 (default) : no weights will be applied
if 1 : classification only, will balanced class with inverse frequency
if dict : keys are corresponding class values are sample weights
if iterable : list or np array must be of length equal to nb elements</p>
<blockquote>
<div><p>in the training set</p>
</div></blockquote>
</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many samples per batch to load</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – how many subprocesses to use for data loading. 0 means that the data
will be loaded in the main process</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – set to True to drop the last incomplete batch, if the dataset size is not
divisible by the batch size. If False and the size of dataset is not
divisible by the batch size, then the last batch will be smaller</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to pin GPU memory during training</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>train_dataloader, valid_dataloader</strong> – Training and validation dataloaders</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.DataLoader, torch.DataLoader</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.pretraining_utils.validate_eval_set">
<code class="sig-prename descclassname">pytorch_tabnet.pretraining_utils.</code><code class="sig-name descname">validate_eval_set</code><span class="sig-paren">(</span><em class="sig-param">eval_set</em>, <em class="sig-param">eval_name</em>, <em class="sig-param">X_train</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining_utils.html#validate_eval_set"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining_utils.validate_eval_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if the shapes of eval_set are compatible with X_train.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eval_set</strong> (<em>List of numpy array</em>) – The list evaluation set.
The last one is used for early stopping</p></li>
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train owned products</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>eval_names</strong> – Validated list of eval_names.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pytorch_tabnet.augmentations">
<span id="pytorch-tabnet-augmentations-module"></span><h2>pytorch_tabnet.augmentations module<a class="headerlink" href="#module-pytorch_tabnet.augmentations" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.augmentations.ClassificationSMOTE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.augmentations.</code><code class="sig-name descname">ClassificationSMOTE</code><span class="sig-paren">(</span><em class="sig-param">device_name='auto'</em>, <em class="sig-param">p=0.8</em>, <em class="sig-param">alpha=0.5</em>, <em class="sig-param">beta=0.5</em>, <em class="sig-param">seed=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/augmentations.html#ClassificationSMOTE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.augmentations.ClassificationSMOTE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Apply SMOTE for classification tasks.</p>
<p>This will average a percentage p of the elements in the batch with other elements.
The target will stay unchanged and keep the value of the most important row in the mix.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.augmentations.RegressionSMOTE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.augmentations.</code><code class="sig-name descname">RegressionSMOTE</code><span class="sig-paren">(</span><em class="sig-param">device_name='auto'</em>, <em class="sig-param">p=0.8</em>, <em class="sig-param">alpha=0.5</em>, <em class="sig-param">beta=0.5</em>, <em class="sig-param">seed=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/augmentations.html#RegressionSMOTE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.augmentations.RegressionSMOTE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Apply SMOTE</p>
<p>This will average a percentage p of the elements in the batch with other elements.
The target will be averaged as well (this might work with binary classification
and certain loss), following a beta distribution.</p>
</dd></dl>

</section>
<section id="module-pytorch_tabnet.tab_network">
<span id="pytorch-tabnet-tab-network-module"></span><h2>pytorch_tabnet.tab_network module<a class="headerlink" href="#module-pytorch_tabnet.tab_network" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.tab_network.AttentiveTransformer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">AttentiveTransformer</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">group_dim</em>, <em class="sig-param">group_matrix</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em>, <em class="sig-param">mask_type='sparsemax'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#AttentiveTransformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.AttentiveTransformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.AttentiveTransformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">priors</em>, <em class="sig-param">processed_feat</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#AttentiveTransformer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.AttentiveTransformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.AttentiveTransformer.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.AttentiveTransformer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.EmbeddingGenerator">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">EmbeddingGenerator</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">cat_dims</em>, <em class="sig-param">cat_idxs</em>, <em class="sig-param">cat_emb_dims</em>, <em class="sig-param">group_matrix</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#EmbeddingGenerator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.EmbeddingGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Classical embeddings generator</p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.EmbeddingGenerator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#EmbeddingGenerator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.EmbeddingGenerator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply embeddings to inputs
Inputs should be (batch_size, input_dim)
Outputs will be of size (batch_size, self.post_embed_dim)</p>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.EmbeddingGenerator.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.EmbeddingGenerator.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.FeatTransformer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">FeatTransformer</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">shared_layers</em>, <em class="sig-param">n_glu_independent</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#FeatTransformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.FeatTransformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.FeatTransformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#FeatTransformer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.FeatTransformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.FeatTransformer.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.FeatTransformer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.GBN">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">GBN</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GBN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GBN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Ghost Batch Normalization
<a class="reference external" href="https://arxiv.org/abs/1705.08741">https://arxiv.org/abs/1705.08741</a></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.GBN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GBN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GBN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.GBN.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.GBN.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.GLU_Block">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">GLU_Block</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">n_glu=2</em>, <em class="sig-param">first=False</em>, <em class="sig-param">shared_layers=None</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GLU_Block"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Block" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Independent GLU block, specific to each step</p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.GLU_Block.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GLU_Block.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Block.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.GLU_Block.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Block.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.GLU_Layer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">GLU_Layer</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">fc=None</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GLU_Layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.GLU_Layer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#GLU_Layer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Layer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.GLU_Layer.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.GLU_Layer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.RandomObfuscator">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">RandomObfuscator</code><span class="sig-paren">(</span><em class="sig-param">pretraining_ratio</em>, <em class="sig-param">group_matrix</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#RandomObfuscator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.RandomObfuscator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Create and applies obfuscation masks.
The obfuscation is done at group level to match attention.</p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.RandomObfuscator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#RandomObfuscator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.RandomObfuscator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate random obfuscation mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>masked input and obfuscated variables.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.RandomObfuscator.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.RandomObfuscator.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.TabNet">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">TabNet</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">n_d=8</em>, <em class="sig-param">n_a=8</em>, <em class="sig-param">n_steps=3</em>, <em class="sig-param">gamma=1.3</em>, <em class="sig-param">cat_idxs=[]</em>, <em class="sig-param">cat_dims=[]</em>, <em class="sig-param">cat_emb_dim=1</em>, <em class="sig-param">n_independent=2</em>, <em class="sig-param">n_shared=2</em>, <em class="sig-param">epsilon=1e-15</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em>, <em class="sig-param">mask_type='sparsemax'</em>, <em class="sig-param">group_attention_matrix=[]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNet.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNet.forward_masks">
<code class="sig-name descname">forward_masks</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNet.forward_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNet.forward_masks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.TabNet.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNet.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.TabNetDecoder">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">TabNetDecoder</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">n_d=8</em>, <em class="sig-param">n_steps=3</em>, <em class="sig-param">n_independent=1</em>, <em class="sig-param">n_shared=1</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">steps_output</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.TabNetDecoder.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetDecoder.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.TabNetEncoder">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">TabNetEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">n_d=8</em>, <em class="sig-param">n_a=8</em>, <em class="sig-param">n_steps=3</em>, <em class="sig-param">gamma=1.3</em>, <em class="sig-param">n_independent=2</em>, <em class="sig-param">n_shared=2</em>, <em class="sig-param">epsilon=1e-15</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em>, <em class="sig-param">mask_type='sparsemax'</em>, <em class="sig-param">group_attention_matrix=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">prior=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetEncoder.forward_masks">
<code class="sig-name descname">forward_masks</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetEncoder.forward_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetEncoder.forward_masks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.TabNetEncoder.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetEncoder.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.TabNetNoEmbeddings">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">TabNetNoEmbeddings</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">n_d=8</em>, <em class="sig-param">n_a=8</em>, <em class="sig-param">n_steps=3</em>, <em class="sig-param">gamma=1.3</em>, <em class="sig-param">n_independent=2</em>, <em class="sig-param">n_shared=2</em>, <em class="sig-param">epsilon=1e-15</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em>, <em class="sig-param">mask_type='sparsemax'</em>, <em class="sig-param">group_attention_matrix=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetNoEmbeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetNoEmbeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetNoEmbeddings.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetNoEmbeddings.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetNoEmbeddings.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetNoEmbeddings.forward_masks">
<code class="sig-name descname">forward_masks</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetNoEmbeddings.forward_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetNoEmbeddings.forward_masks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.TabNetNoEmbeddings.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetNoEmbeddings.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_network.TabNetPretraining">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">TabNetPretraining</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">pretraining_ratio=0.2</em>, <em class="sig-param">n_d=8</em>, <em class="sig-param">n_a=8</em>, <em class="sig-param">n_steps=3</em>, <em class="sig-param">gamma=1.3</em>, <em class="sig-param">cat_idxs=[]</em>, <em class="sig-param">cat_dims=[]</em>, <em class="sig-param">cat_emb_dim=1</em>, <em class="sig-param">n_independent=2</em>, <em class="sig-param">n_shared=2</em>, <em class="sig-param">epsilon=1e-15</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">momentum=0.02</em>, <em class="sig-param">mask_type='sparsemax'</em>, <em class="sig-param">n_shared_decoder=1</em>, <em class="sig-param">n_indep_decoder=1</em>, <em class="sig-param">group_attention_matrix=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetPretraining"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetPretraining" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetPretraining.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetPretraining.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetPretraining.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Returns: res, embedded_x, obf_vars</dt><dd><p>res : output of reconstruction
embedded_x : embedded input
obf_vars : which variable where obfuscated</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_network.TabNetPretraining.forward_masks">
<code class="sig-name descname">forward_masks</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#TabNetPretraining.forward_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetPretraining.forward_masks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_network.TabNetPretraining.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_network.TabNetPretraining.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.tab_network.initialize_glu">
<code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">initialize_glu</code><span class="sig-paren">(</span><em class="sig-param">module</em>, <em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#initialize_glu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.initialize_glu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.tab_network.initialize_non_glu">
<code class="sig-prename descclassname">pytorch_tabnet.tab_network.</code><code class="sig-name descname">initialize_non_glu</code><span class="sig-paren">(</span><em class="sig-param">module</em>, <em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_network.html#initialize_non_glu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_network.initialize_non_glu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-pytorch_tabnet.metrics">
<span id="pytorch-tabnet-metrics-module"></span><h2>pytorch_tabnet.metrics module<a class="headerlink" href="#module-pytorch_tabnet.metrics" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.metrics.AUC">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">AUC</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#AUC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.AUC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>AUC.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.Accuracy">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">Accuracy</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#Accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.Accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Accuracy.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.BalancedAccuracy">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">BalancedAccuracy</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#BalancedAccuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.BalancedAccuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Balanced Accuracy.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.LogLoss">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">LogLoss</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#LogLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.LogLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>LogLoss.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.MAE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">MAE</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#MAE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.MAE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Mean Absolute Error.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.MSE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">MSE</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#MSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.MSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Mean Squared Error.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.Metric">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">Metric</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#Metric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.Metric" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.metrics.Metric.get_metrics_by_names">
<em class="property">classmethod </em><code class="sig-name descname">get_metrics_by_names</code><span class="sig-paren">(</span><em class="sig-param">names</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#Metric.get_metrics_by_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.Metric.get_metrics_by_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Get list of metric classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cls</strong> (<a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><em>Metric</em></a>) – Metric class.</p></li>
<li><p><strong>names</strong> (<em>list</em>) – List of metric names.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>metrics</strong> – List of metric classes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.MetricContainer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">MetricContainer</code><span class="sig-paren">(</span><em class="sig-param">metric_names: List[str], prefix: str = ''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#MetricContainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.MetricContainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Container holding a list of metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric_names</strong> (<em>list of str</em>) – List of metric names.</p></li>
<li><p><strong>prefix</strong> (<em>str</em>) – Prefix of metric names.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytorch_tabnet.metrics.MetricContainer.metric_names">
<code class="sig-name descname">metric_names</code><em class="property">: List[str]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.metrics.MetricContainer.metric_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.metrics.MetricContainer.prefix">
<code class="sig-name descname">prefix</code><em class="property">: str</em><em class="property"> = ''</em><a class="headerlink" href="#pytorch_tabnet.metrics.MetricContainer.prefix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.RMSE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">RMSE</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#RMSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.RMSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Root Mean Squared Error.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.RMSLE">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">RMSLE</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#RMSLE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.RMSLE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Root Mean squared logarithmic error regression loss.
Scikit-implementation:
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html</a>
Note: In order to avoid error, negative predictions are clipped to 0.
This means that you should clip negative predictions manually after calling predict.</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.UnsupMetricContainer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">UnsupMetricContainer</code><span class="sig-paren">(</span><em class="sig-param">metric_names: List[str], prefix: str = ''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#UnsupMetricContainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.UnsupMetricContainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Container holding a list of metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_pred</strong> (<em>torch.Tensor</em><em> or </em><em>np.array</em>) – Reconstructed prediction (with embeddings)</p></li>
<li><p><strong>embedded_x</strong> (<em>torch.Tensor</em>) – Original input embedded by network</p></li>
<li><p><strong>obf_vars</strong> (<em>torch.Tensor</em>) – Binary mask for obfuscated variables.
1 means the variables was obfuscated so reconstruction is based on this.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytorch_tabnet.metrics.UnsupMetricContainer.metric_names">
<code class="sig-name descname">metric_names</code><em class="property">: List[str]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.metrics.UnsupMetricContainer.metric_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.metrics.UnsupMetricContainer.prefix">
<code class="sig-name descname">prefix</code><em class="property">: str</em><em class="property"> = ''</em><a class="headerlink" href="#pytorch_tabnet.metrics.UnsupMetricContainer.prefix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.metrics.UnsupervisedLoss">
<code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">UnsupervisedLoss</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">embedded_x</em>, <em class="sig-param">obf_vars</em>, <em class="sig-param">eps=1e-09</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#UnsupervisedLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.UnsupervisedLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements unsupervised loss function.
This differs from orginal paper as it’s scaled to be batch size independent
and number of features reconstructed independent (by taking the mean)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_pred</strong> (<em>torch.Tensor</em><em> or </em><em>np.array</em>) – Reconstructed prediction (with embeddings)</p></li>
<li><p><strong>embedded_x</strong> (<em>torch.Tensor</em>) – Original input embedded by network</p></li>
<li><p><strong>obf_vars</strong> (<em>torch.Tensor</em>) – Binary mask for obfuscated variables.
1 means the variable was obfuscated so reconstruction is based on this.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – A small floating point to avoid ZeroDivisionError
This can happen in degenerated case when a feature has only one value</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – Unsupervised loss, average value over batch samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch float</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.metrics.UnsupervisedLossNumpy">
<code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">UnsupervisedLossNumpy</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">embedded_x</em>, <em class="sig-param">obf_vars</em>, <em class="sig-param">eps=1e-09</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#UnsupervisedLossNumpy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.UnsupervisedLossNumpy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.UnsupervisedMetric">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">UnsupervisedMetric</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#UnsupervisedMetric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.UnsupervisedMetric" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Unsupervised metric</p>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.metrics.UnsupervisedNumpyMetric">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">UnsupervisedNumpyMetric</code><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#UnsupervisedNumpyMetric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.UnsupervisedNumpyMetric" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.metrics.Metric" title="pytorch_tabnet.metrics.Metric"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.metrics.Metric</span></code></a></p>
<p>Unsupervised metric</p>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.metrics.check_metrics">
<code class="sig-prename descclassname">pytorch_tabnet.metrics.</code><code class="sig-name descname">check_metrics</code><span class="sig-paren">(</span><em class="sig-param">metrics</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/metrics.html#check_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.metrics.check_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if custom metrics are provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>metrics</strong> (<em>list of str</em><em> or </em><em>classes</em>) – List with built-in metrics (str) or custom metrics (classes).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>val_metrics</strong> – List of metric names.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pytorch_tabnet.tab_model">
<span id="pytorch-tabnet-tab-model-module"></span><h2>pytorch_tabnet.tab_model module<a class="headerlink" href="#module-pytorch_tabnet.tab_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_model.</code><code class="sig-name descname">TabNetClassifier</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em>, <em class="sig-param">n_shared_decoder: int = 1</em>, <em class="sig-param">n_indep_decoder: int = 1</em>, <em class="sig-param">grouped_features: List[List[int]] = &lt;factory&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.abstract_model.TabModel" title="pytorch_tabnet.abstract_model.TabModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.abstract_model.TabModel</span></code></a></p>
<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">y_true</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_score</strong> (a :tensor: <cite>torch.Tensor</cite>) – Score matrix</p></li>
<li><p><strong>y_true</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.grouped_features">
<code class="sig-name descname">grouped_features</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.grouped_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.predict_func">
<code class="sig-name descname">predict_func</code><span class="sig-paren">(</span><em class="sig-param">outputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.predict_func"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.predict_func" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions for classification on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite> or matrix: <cite>scipy.sparse.csr_matrix</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>res</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.prepare_target">
<code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.stack_batches">
<code class="sig-name descname">stack_batches</code><span class="sig-paren">(</span><em class="sig-param">list_y_true</em>, <em class="sig-param">list_y_score</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.stack_batches"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.stack_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.update_fit_params">
<code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetClassifier.weight_updater">
<code class="sig-name descname">weight_updater</code><span class="sig-paren">(</span><em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetClassifier.weight_updater"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetClassifier.weight_updater" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates weights dictionary according to target_mapper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dict</em>) – Given weights for balancing training.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Same bool if weights are bool, updated dict otherwise.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bool or dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.tab_model.</code><code class="sig-name descname">TabNetRegressor</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em>, <em class="sig-param">n_shared_decoder: int = 1</em>, <em class="sig-param">n_indep_decoder: int = 1</em>, <em class="sig-param">grouped_features: List[List[int]] = &lt;factory&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.abstract_model.TabModel" title="pytorch_tabnet.abstract_model.TabModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.abstract_model.TabModel</span></code></a></p>
<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">y_true</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_score</strong> (a :tensor: <cite>torch.Tensor</cite>) – Score matrix</p></li>
<li><p><strong>y_true</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.grouped_features">
<code class="sig-name descname">grouped_features</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.grouped_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.predict_func">
<code class="sig-name descname">predict_func</code><span class="sig-paren">(</span><em class="sig-param">outputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.predict_func"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.predict_func" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.prepare_target">
<code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.stack_batches">
<code class="sig-name descname">stack_batches</code><span class="sig-paren">(</span><em class="sig-param">list_y_true</em>, <em class="sig-param">list_y_score</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.stack_batches"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.stack_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.tab_model.TabNetRegressor.update_fit_params">
<code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/tab_model.html#TabNetRegressor.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.tab_model.TabNetRegressor.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-pytorch_tabnet.sparsemax">
<span id="pytorch-tabnet-sparsemax-module"></span><h2>pytorch_tabnet.sparsemax module<a class="headerlink" href="#module-pytorch_tabnet.sparsemax" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.sparsemax.Entmax15">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">Entmax15</code><span class="sig-paren">(</span><em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmax15.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.sparsemax.Entmax15.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.sparsemax.Entmax15Function">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">Entmax15Function</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15Function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15Function" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>An implementation of exact Entmax with alpha=1.5 (B. Peters, V. Niculae, A. Martins). See
:cite:<a href="#id1"><span class="problematic" id="id2">`</span></a><a class="reference external" href="https://arxiv.org/abs/1905.05702">https://arxiv.org/abs/1905.05702</a> for detailed description.
Source: <a class="reference external" href="https://github.com/deep-spin/entmax">https://github.com/deep-spin/entmax</a></p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmax15Function.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15Function.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15Function.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmax15Function.forward" title="pytorch_tabnet.sparsemax.Entmax15Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmax15Function.forward" title="pytorch_tabnet.sparsemax.Entmax15Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmax15Function.backward" title="pytorch_tabnet.sparsemax.Entmax15Function.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmax15Function.forward" title="pytorch_tabnet.sparsemax.Entmax15Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmax15Function.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">input</em>, <em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmax15Function.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmax15Function.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward()</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward()</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.sparsemax.Entmoid15">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">Entmoid15</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmoid15"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmoid15" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>A highly optimized equivalent of lambda x: Entmax15([x, 0])</p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmoid15.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmoid15.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmoid15.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmoid15.forward" title="pytorch_tabnet.sparsemax.Entmoid15.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmoid15.forward" title="pytorch_tabnet.sparsemax.Entmoid15.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmoid15.backward" title="pytorch_tabnet.sparsemax.Entmoid15.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#pytorch_tabnet.sparsemax.Entmoid15.forward" title="pytorch_tabnet.sparsemax.Entmoid15.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Entmoid15.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Entmoid15.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Entmoid15.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward()</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward()</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.sparsemax.Sparsemax">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">Sparsemax</code><span class="sig-paren">(</span><em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Sparsemax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Sparsemax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.Sparsemax.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#Sparsemax.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.Sparsemax.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.sparsemax.Sparsemax.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.sparsemax.Sparsemax.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.sparsemax.SparsemaxFunction">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">SparsemaxFunction</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#SparsemaxFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.SparsemaxFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>An implementation of sparsemax (Martins &amp; Astudillo, 2016). See
<a href="#id3"><span class="problematic" id="id4">:cite:`DBLP:journals/corr/MartinsA16`</span></a> for detailed description.
By Ben Peters and Vlad Niculae</p>
<dl class="method">
<dt id="pytorch_tabnet.sparsemax.SparsemaxFunction.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#SparsemaxFunction.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.forward" title="pytorch_tabnet.sparsemax.SparsemaxFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.forward" title="pytorch_tabnet.sparsemax.SparsemaxFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.backward" title="pytorch_tabnet.sparsemax.SparsemaxFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.forward" title="pytorch_tabnet.sparsemax.SparsemaxFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.sparsemax.SparsemaxFunction.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">input</em>, <em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/sparsemax.html#SparsemaxFunction.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.sparsemax.SparsemaxFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>sparsemax: normalizing sparse transform (a la softmax)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<em>torch.autograd.function._ContextMethodMixin</em>) – </p></li>
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – any shape</p></li>
<li><p><strong>dim</strong> (<em>int</em>) – dimension along which to apply sparsemax</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>output</strong> – same shape as input</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.sparsemax.entmax15">
<code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">entmax15</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pytorch_tabnet.sparsemax.entmax15" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.sparsemax.entmoid15">
<code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">entmoid15</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pytorch_tabnet.sparsemax.entmoid15" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.sparsemax.sparsemax">
<code class="sig-prename descclassname">pytorch_tabnet.sparsemax.</code><code class="sig-name descname">sparsemax</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pytorch_tabnet.sparsemax.sparsemax" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-pytorch_tabnet.callbacks">
<span id="pytorch-tabnet-callbacks-module"></span><h2>pytorch_tabnet.callbacks module<a class="headerlink" href="#module-pytorch_tabnet.callbacks" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.callbacks.Callback">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">Callback</code><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Abstract base class used to build new callbacks.</p>
<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_batch_begin">
<code class="sig-name descname">on_batch_begin</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_batch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_batch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_batch_end">
<code class="sig-name descname">on_batch_end</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_epoch_begin">
<code class="sig-name descname">on_epoch_begin</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_epoch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_epoch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_train_begin">
<code class="sig-name descname">on_train_begin</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_train_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_train_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.on_train_end">
<code class="sig-name descname">on_train_end</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.on_train_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">params</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.set_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.set_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.Callback.set_trainer">
<code class="sig-name descname">set_trainer</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#Callback.set_trainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.Callback.set_trainer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.callbacks.CallbackContainer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">CallbackContainer</code><span class="sig-paren">(</span><em class="sig-param">callbacks: List[pytorch_tabnet.callbacks.Callback] = &lt;factory&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Container holding a list of callbacks.</p>
<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.append">
<code class="sig-name descname">append</code><span class="sig-paren">(</span><em class="sig-param">callback</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.append" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.callbacks">
<code class="sig-name descname">callbacks</code><em class="property">: List[Callback]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.callbacks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_batch_begin">
<code class="sig-name descname">on_batch_begin</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_batch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_batch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_batch_end">
<code class="sig-name descname">on_batch_end</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_epoch_begin">
<code class="sig-name descname">on_epoch_begin</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_epoch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_epoch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_train_begin">
<code class="sig-name descname">on_train_begin</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_train_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_train_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.on_train_end">
<code class="sig-name descname">on_train_end</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.on_train_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">params</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.set_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.set_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.CallbackContainer.set_trainer">
<code class="sig-name descname">set_trainer</code><span class="sig-paren">(</span><em class="sig-param">trainer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#CallbackContainer.set_trainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.CallbackContainer.set_trainer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.callbacks.EarlyStopping">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">EarlyStopping</code><span class="sig-paren">(</span><em class="sig-param">early_stopping_metric: str</em>, <em class="sig-param">is_maximize: bool</em>, <em class="sig-param">tol: float = 0.0</em>, <em class="sig-param">patience: int = 5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#EarlyStopping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.callbacks.Callback" title="pytorch_tabnet.callbacks.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.callbacks.Callback</span></code></a></p>
<p>EarlyStopping callback to exit the training loop if early_stopping_metric
does not improve by a certain amount for a certain
number of epochs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>early_stopping_metric</strong> (<em>str</em>) – Early stopping metric name</p></li>
<li><p><strong>is_maximize</strong> (<em>bool</em>) – Whether to maximize or not early_stopping_metric</p></li>
<li><p><strong>tol</strong> (<em>float</em>) – minimum change in monitored value to qualify as improvement.
This number should be positive.</p></li>
<li><p><strong>patience</strong> (<em>integer</em>) – number of epochs to wait for improvement before terminating.
the counter be reset after each improvement</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.early_stopping_metric">
<code class="sig-name descname">early_stopping_metric</code><em class="property">: str</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.early_stopping_metric" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.is_maximize">
<code class="sig-name descname">is_maximize</code><em class="property">: bool</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.is_maximize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#EarlyStopping.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.on_train_end">
<code class="sig-name descname">on_train_end</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#EarlyStopping.on_train_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.patience">
<code class="sig-name descname">patience</code><em class="property">: int</em><em class="property"> = 5</em><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.patience" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.EarlyStopping.tol">
<code class="sig-name descname">tol</code><em class="property">: float</em><em class="property"> = 0.0</em><a class="headerlink" href="#pytorch_tabnet.callbacks.EarlyStopping.tol" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.callbacks.History">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">History</code><span class="sig-paren">(</span><em class="sig-param">trainer: Any</em>, <em class="sig-param">verbose: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.callbacks.Callback" title="pytorch_tabnet.callbacks.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.callbacks.Callback</span></code></a></p>
<p>Callback that records events into a <cite>History</cite> object.
This callback is automatically applied to
every SuperModule.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainer</strong> (<em>DeepRecoModel</em>) – Model class to train</p></li>
<li><p><strong>verbose</strong> (<em>int</em>) – Print results every verbose iteration</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pytorch_tabnet.callbacks.History.on_batch_end">
<code class="sig-name descname">on_batch_end</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History.on_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History.on_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.History.on_epoch_begin">
<code class="sig-name descname">on_epoch_begin</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History.on_epoch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History.on_epoch_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.History.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.History.on_train_begin">
<code class="sig-name descname">on_train_begin</code><span class="sig-paren">(</span><em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#History.on_train_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.History.on_train_begin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.History.trainer">
<code class="sig-name descname">trainer</code><em class="property">: Any</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.History.trainer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.History.verbose">
<code class="sig-name descname">verbose</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.callbacks.History.verbose" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.callbacks.</code><code class="sig-name descname">LRSchedulerCallback</code><span class="sig-paren">(</span><em class="sig-param">scheduler_fn: Any</em>, <em class="sig-param">optimizer: Any</em>, <em class="sig-param">scheduler_params: dict</em>, <em class="sig-param">early_stopping_metric: str</em>, <em class="sig-param">is_batch_level: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#LRSchedulerCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.callbacks.Callback" title="pytorch_tabnet.callbacks.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.callbacks.Callback</span></code></a></p>
<p>Wrapper for most torch scheduler functions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scheduler_fn</strong> (<em>torch.optim.lr_scheduler</em>) – Torch scheduling class</p></li>
<li><p><strong>scheduler_params</strong> (<em>dict</em>) – Dictionnary containing all parameters for the scheduler_fn</p></li>
<li><p><strong>is_batch_level</strong> (<em>bool</em><em> (</em><em>default = False</em><em>)</em>) – If set to False : lr updates will happen at every epoch
If set to True : lr updates happen at every batch
Set this to True for OneCycleLR for example</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.early_stopping_metric">
<code class="sig-name descname">early_stopping_metric</code><em class="property">: str</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.early_stopping_metric" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.is_batch_level">
<code class="sig-name descname">is_batch_level</code><em class="property">: bool</em><em class="property"> = False</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.is_batch_level" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.on_batch_end">
<code class="sig-name descname">on_batch_end</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#LRSchedulerCallback.on_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.on_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">epoch</em>, <em class="sig-param">logs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/callbacks.html#LRSchedulerCallback.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.optimizer">
<code class="sig-name descname">optimizer</code><em class="property">: Any</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.optimizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.scheduler_fn">
<code class="sig-name descname">scheduler_fn</code><em class="property">: Any</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.scheduler_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.callbacks.LRSchedulerCallback.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property">: dict</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.callbacks.LRSchedulerCallback.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-pytorch_tabnet.abstract_model">
<span id="pytorch-tabnet-abstract-model-module"></span><h2>pytorch_tabnet.abstract_model module<a class="headerlink" href="#module-pytorch_tabnet.abstract_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.abstract_model.TabModel">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.abstract_model.</code><code class="sig-name descname">TabModel</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em>, <em class="sig-param">n_shared_decoder: int = 1</em>, <em class="sig-param">n_indep_decoder: int = 1</em>, <em class="sig-param">grouped_features: List[List[int]] = &lt;factory&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Class for TabNet model.</p>
<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property">: List[int]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.cat_emb_dim">
<code class="sig-name descname">cat_emb_dim</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.cat_emb_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property">: List[int]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.clip_value">
<code class="sig-name descname">clip_value</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.clip_value" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.compute_loss">
<em class="property">abstract </em><code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">y_score</em>, <em class="sig-param">y_true</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_score</strong> (a :tensor: <cite>torch.Tensor</cite>) – Score matrix</p></li>
<li><p><strong>y_true</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.device_name">
<code class="sig-name descname">device_name</code><em class="property">: str</em><em class="property"> = 'auto'</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.device_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.epsilon">
<code class="sig-name descname">epsilon</code><em class="property">: float</em><em class="property"> = 1e-15</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.epsilon" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.explain">
<code class="sig-name descname">explain</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">normalize=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.explain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.explain" title="Permalink to this definition">¶</a></dt>
<dd><p>Return local explanation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (tensor: <cite>torch.Tensor</cite> or matrix: <cite>scipy.sparse.csr_matrix</cite>) – Input data</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em> (</em><em>default False</em><em>)</em>) – Wheter to normalize so that sum of features are equal to 1</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>M_explain</strong> (<em>matrix</em>) – Importance per sample, per columns.</p></li>
<li><p><strong>masks</strong> (<em>matrix</em>) – Sparse matrix showing attention masks used by network.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set=None</em>, <em class="sig-param">eval_name=None</em>, <em class="sig-param">eval_metric=None</em>, <em class="sig-param">loss_fn=None</em>, <em class="sig-param">weights=0</em>, <em class="sig-param">max_epochs=100</em>, <em class="sig-param">patience=10</em>, <em class="sig-param">batch_size=1024</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">num_workers=0</em>, <em class="sig-param">drop_last=True</em>, <em class="sig-param">callbacks=None</em>, <em class="sig-param">pin_memory=True</em>, <em class="sig-param">from_unsupervised=None</em>, <em class="sig-param">warm_start=False</em>, <em class="sig-param">augmentations=None</em>, <em class="sig-param">compute_importance=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train a neural network stored in self.network
Using train_dataloader for training data and
valid_dataloader for validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).
The last one is used for early stopping</p></li>
<li><p><strong>eval_name</strong> (<em>list of str</em>) – List of eval set names.</p></li>
<li><p><strong>eval_metric</strong> (<em>list of str</em>) – List of evaluation metrics.
The last metric is used for early stopping.</p></li>
<li><p><strong>loss_fn</strong> (<em>callable</em><em> or </em><em>None</em>) – a PyTorch loss function</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing
dict for custom weights per class</p></li>
<li><p><strong>max_epochs</strong> (<em>int</em>) – Maximum number of epochs during training</p></li>
<li><p><strong>patience</strong> (<em>int</em>) – Number of consecutive non improving epoch before early stopping</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Training batch size</p></li>
<li><p><strong>virtual_batch_size</strong> (<em>int</em>) – Batch size for Ghost Batch Normalization (virtual_batch_size &lt; batch_size)</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers used in torch.utils.data.DataLoader</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – Whether to drop last batch during training</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback function</em>) – List of custom callbacks</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to set pin_memory to True or False during training</p></li>
<li><p><strong>from_unsupervised</strong> (<em>unsupervised trained model</em>) – Use a previously self supervised model as starting weights</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em>) – If True, current model parameters are used to start training</p></li>
<li><p><strong>compute_importance</strong> (<em>bool</em>) – Whether to compute feature importance</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.gamma">
<code class="sig-name descname">gamma</code><em class="property">: float</em><em class="property"> = 1.3</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.gamma" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.grouped_features">
<code class="sig-name descname">grouped_features</code><em class="property">: List[List[int]]</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.grouped_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.input_dim">
<code class="sig-name descname">input_dim</code><em class="property">: int</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.input_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.lambda_sparse">
<code class="sig-name descname">lambda_sparse</code><em class="property">: float</em><em class="property"> = 0.001</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.lambda_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.load_class_attrs">
<code class="sig-name descname">load_class_attrs</code><span class="sig-paren">(</span><em class="sig-param">class_attrs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.load_class_attrs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.load_class_attrs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">filepath</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.load_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load TabNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filepath</strong> (<em>str</em>) – Path of the model.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.load_weights_from_unsupervised">
<code class="sig-name descname">load_weights_from_unsupervised</code><span class="sig-paren">(</span><em class="sig-param">unsupervised_model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.load_weights_from_unsupervised"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.load_weights_from_unsupervised" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.mask_type">
<code class="sig-name descname">mask_type</code><em class="property">: str</em><em class="property"> = 'sparsemax'</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.mask_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.momentum">
<code class="sig-name descname">momentum</code><em class="property">: float</em><em class="property"> = 0.02</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.momentum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_a">
<code class="sig-name descname">n_a</code><em class="property">: int</em><em class="property"> = 8</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_a" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_d">
<code class="sig-name descname">n_d</code><em class="property">: int</em><em class="property"> = 8</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_indep_decoder">
<code class="sig-name descname">n_indep_decoder</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_indep_decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_independent">
<code class="sig-name descname">n_independent</code><em class="property">: int</em><em class="property"> = 2</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_independent" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_shared">
<code class="sig-name descname">n_shared</code><em class="property">: int</em><em class="property"> = 2</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_shared_decoder">
<code class="sig-name descname">n_shared_decoder</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_shared_decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.n_steps">
<code class="sig-name descname">n_steps</code><em class="property">: int</em><em class="property"> = 3</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.n_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.optimizer_fn">
<code class="sig-name descname">optimizer_fn</code><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.optimizer_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.adam.Adam</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property">: Dict</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.output_dim">
<code class="sig-name descname">output_dim</code><em class="property">: int</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.output_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite> or matrix: <cite>scipy.sparse.csr_matrix</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>predictions</strong> – Predictions of the regression problem</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.prepare_target">
<em class="property">abstract </em><code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.save_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saving TabNet model in two distinct files.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>str</em>) – Path of the model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>input filepath with “.zip” appended</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.scheduler_fn">
<code class="sig-name descname">scheduler_fn</code><em class="property">: Any</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.scheduler_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property">: Dict</em><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.seed">
<code class="sig-name descname">seed</code><em class="property">: int</em><em class="property"> = 0</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.abstract_model.TabModel.update_fit_params">
<em class="property">abstract </em><code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/abstract_model.html#TabModel.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.abstract_model.TabModel.verbose">
<code class="sig-name descname">verbose</code><em class="property">: int</em><em class="property"> = 1</em><a class="headerlink" href="#pytorch_tabnet.abstract_model.TabModel.verbose" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-pytorch_tabnet.pretraining">
<span id="pytorch-tabnet-pretraining-module"></span><h2>pytorch_tabnet.pretraining module<a class="headerlink" href="#module-pytorch_tabnet.pretraining" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.pretraining.</code><code class="sig-name descname">TabNetPretrainer</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em>, <em class="sig-param">n_shared_decoder: int = 1</em>, <em class="sig-param">n_indep_decoder: int = 1</em>, <em class="sig-param">grouped_features: List[List[int]] = &lt;factory&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining.html#TabNetPretrainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.abstract_model.TabModel" title="pytorch_tabnet.abstract_model.TabModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.abstract_model.TabModel</span></code></a></p>
<dl class="attribute">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">output</em>, <em class="sig-param">embedded_x</em>, <em class="sig-param">obf_vars</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining.html#TabNetPretrainer.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_score</strong> (a :tensor: <cite>torch.Tensor</cite>) – Score matrix</p></li>
<li><p><strong>y_true</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">eval_set=None</em>, <em class="sig-param">eval_name=None</em>, <em class="sig-param">loss_fn=None</em>, <em class="sig-param">pretraining_ratio=0.5</em>, <em class="sig-param">weights=0</em>, <em class="sig-param">max_epochs=100</em>, <em class="sig-param">patience=10</em>, <em class="sig-param">batch_size=1024</em>, <em class="sig-param">virtual_batch_size=128</em>, <em class="sig-param">num_workers=0</em>, <em class="sig-param">drop_last=True</em>, <em class="sig-param">callbacks=None</em>, <em class="sig-param">pin_memory=True</em>, <em class="sig-param">warm_start=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining.html#TabNetPretrainer.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train a neural network stored in self.network
Using train_dataloader for training data and
valid_dataloader for validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set to reconstruct in self supervision</p></li>
<li><p><strong>eval_set</strong> (<em>list of np.array</em>) – List of evaluation set
The last one is used for early stopping</p></li>
<li><p><strong>eval_name</strong> (<em>list of str</em>) – List of eval set names.</p></li>
<li><p><strong>eval_metric</strong> (<em>list of str</em>) – List of evaluation metrics.
The last metric is used for early stopping.</p></li>
<li><p><strong>loss_fn</strong> (<em>callable</em><em> or </em><em>None</em>) – a PyTorch loss function
should be left to None for self supervised and non experts</p></li>
<li><p><strong>pretraining_ratio</strong> (<em>float</em>) – Between 0 and 1, percentage of feature to mask for reconstruction</p></li>
<li><p><strong>weights</strong> (<em>np.array</em>) – Sampling weights for each example.</p></li>
<li><p><strong>max_epochs</strong> (<em>int</em>) – Maximum number of epochs during training</p></li>
<li><p><strong>patience</strong> (<em>int</em>) – Number of consecutive non improving epoch before early stopping</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Training batch size</p></li>
<li><p><strong>virtual_batch_size</strong> (<em>int</em>) – Batch size for Ghost Batch Normalization (virtual_batch_size &lt; batch_size)</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers used in torch.utils.data.DataLoader</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – Whether to drop last batch during training</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback function</em>) – List of custom callbacks</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to set pin_memory to True or False during training</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.grouped_features">
<code class="sig-name descname">grouped_features</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.grouped_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining.html#TabNetPretrainer.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite> or matrix: <cite>scipy.sparse.csr_matrix</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>predictions</strong> – Predictions of the regression problem</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.prepare_target">
<code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining.html#TabNetPretrainer.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.stack_batches">
<code class="sig-name descname">stack_batches</code><span class="sig-paren">(</span><em class="sig-param">list_output</em>, <em class="sig-param">list_embedded_x</em>, <em class="sig-param">list_obfuscation</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining.html#TabNetPretrainer.stack_batches"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.stack_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.pretraining.TabNetPretrainer.update_fit_params">
<code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/pretraining.html#TabNetPretrainer.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.pretraining.TabNetPretrainer.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-pytorch_tabnet.utils">
<span id="pytorch-tabnet-utils-module"></span><h2>pytorch_tabnet.utils module<a class="headerlink" href="#module-pytorch_tabnet.utils" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.utils.ComplexEncoder">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">ComplexEncoder</code><span class="sig-paren">(</span><em class="sig-param">*</em>, <em class="sig-param">skipkeys=False</em>, <em class="sig-param">ensure_ascii=True</em>, <em class="sig-param">check_circular=True</em>, <em class="sig-param">allow_nan=True</em>, <em class="sig-param">sort_keys=False</em>, <em class="sig-param">indent=None</em>, <em class="sig-param">separators=None</em>, <em class="sig-param">default=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#ComplexEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.ComplexEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">json.encoder.JSONEncoder</span></code></p>
<dl class="method">
<dt id="pytorch_tabnet.utils.ComplexEncoder.default">
<code class="sig-name descname">default</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#ComplexEncoder.default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.ComplexEncoder.default" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement this method in a subclass such that it returns
a serializable object for <code class="docutils literal notranslate"><span class="pre">o</span></code>, or calls the base implementation
(to raise a <code class="docutils literal notranslate"><span class="pre">TypeError</span></code>).</p>
<p>For example, to support arbitrary iterators, you could
implement default like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">default</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">o</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">iterable</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
    <span class="c1"># Let the base class default method raise the TypeError</span>
    <span class="k">return</span> <span class="n">JSONEncoder</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.utils.PredictDataset">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">PredictDataset</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#PredictDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.PredictDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>Format for numpy array</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>2D array</em>) – The input matrix</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.utils.SparsePredictDataset">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">SparsePredictDataset</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#SparsePredictDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.SparsePredictDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>Format for csr_matrix</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>CSR matrix</em>) – The input matrix</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.utils.SparseTorchDataset">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">SparseTorchDataset</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#SparseTorchDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.SparseTorchDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>Format for csr_matrix</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>CSR matrix</em>) – The input matrix</p></li>
<li><p><strong>y</strong> (<em>2D array</em>) – The one-hot encoded target</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="pytorch_tabnet.utils.TorchDataset">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">TorchDataset</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#TorchDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.TorchDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>Format for numpy array</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>2D array</em>) – The input matrix</p></li>
<li><p><strong>y</strong> (<em>2D array</em>) – The one-hot encoded target</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.check_embedding_parameters">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">check_embedding_parameters</code><span class="sig-paren">(</span><em class="sig-param">cat_dims</em>, <em class="sig-param">cat_idxs</em>, <em class="sig-param">cat_emb_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#check_embedding_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.check_embedding_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Check parameters related to embeddings and rearrange them in a unique manner.</p>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.check_input">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">check_input</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#check_input"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.check_input" title="Permalink to this definition">¶</a></dt>
<dd><p>Raise a clear error if X is a pandas dataframe
and check array according to scikit rules</p>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.check_list_groups">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">check_list_groups</code><span class="sig-paren">(</span><em class="sig-param">list_groups</em>, <em class="sig-param">input_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#check_list_groups"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.check_list_groups" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Check that list groups:</dt><dd><ul class="simple">
<li><p>is a list of list</p></li>
<li><p>does not contain twice the same feature in different groups</p></li>
<li><p>does not contain unknown features (&gt;= input_dim)</p></li>
<li><p>does not contain empty groups</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>list_groups</strong> (<em>-</em>) – Each element is a list representing features in the same group.
One feature should appear in maximum one group.
Feature that don’t get assign a group will be in their own group of one feature.</p></li>
<li><p><strong>input_dim</strong> (<em>-</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.check_warm_start">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">check_warm_start</code><span class="sig-paren">(</span><em class="sig-param">warm_start</em>, <em class="sig-param">from_unsupervised</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#check_warm_start"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.check_warm_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Gives a warning about ambiguous usage of the two parameters.</p>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.create_dataloaders">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">create_dataloaders</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">num_workers</em>, <em class="sig-param">drop_last</em>, <em class="sig-param">pin_memory</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#create_dataloaders"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.create_dataloaders" title="Permalink to this definition">¶</a></dt>
<dd><p>Create dataloaders with or without subsampling depending on weights and balanced.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Training data</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Mapped Training targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y)</p></li>
<li><p><strong>weights</strong> (<em>either 0</em><em>, </em><em>1</em><em>, </em><em>dict</em><em> or </em><em>iterable</em>) – <p>if 0 (default) : no weights will be applied
if 1 : classification only, will balanced class with inverse frequency
if dict : keys are corresponding class values are sample weights
if iterable : list or np array must be of length equal to nb elements</p>
<blockquote>
<div><p>in the training set</p>
</div></blockquote>
</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many samples per batch to load</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – how many subprocesses to use for data loading. 0 means that the data
will be loaded in the main process</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – set to True to drop the last incomplete batch, if the dataset size is not
divisible by the batch size. If False and the size of dataset is not
divisible by the batch size, then the last batch will be smaller</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em>) – Whether to pin GPU memory during training</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>train_dataloader, valid_dataloader</strong> – Training and validation dataloaders</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.DataLoader, torch.DataLoader</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.create_explain_matrix">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">create_explain_matrix</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">cat_emb_dim</em>, <em class="sig-param">cat_idxs</em>, <em class="sig-param">post_embed_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#create_explain_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.create_explain_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a computational trick.
In order to rapidly sum importances from same embeddings
to the initial index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – Initial input dim</p></li>
<li><p><strong>cat_emb_dim</strong> (<em>int</em><em> or </em><em>list of int</em>) – if int : size of embedding for all categorical feature
if list of int : size of embedding for each categorical feature</p></li>
<li><p><strong>cat_idxs</strong> (<em>list of int</em>) – Initial position of categorical features</p></li>
<li><p><strong>post_embed_dim</strong> (<em>int</em>) – Post embedding inputs dimension</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>reducing_matrix</strong> – Matrix of dim (post_embed_dim, input_dim)  to performe reduce</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.create_group_matrix">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">create_group_matrix</code><span class="sig-paren">(</span><em class="sig-param">list_groups</em>, <em class="sig-param">input_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#create_group_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.create_group_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the group matrix corresponding to the given list_groups</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>list_groups</strong> (<em>-</em>) – Each element is a list representing features in the same group.
One feature should appear in maximum one group.
Feature that don’t get assigned a group will be in their own group of one feature.</p></li>
<li><p><strong>input_dim</strong> (<em>-</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>- group_matrix</strong> – A matrix of size (n_groups, input_dim)
where m_ij represents the importance of feature j in group i
The rows must some to 1 as each group is equally important a priori.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch matrix</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.create_sampler">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">create_sampler</code><span class="sig-paren">(</span><em class="sig-param">weights</em>, <em class="sig-param">y_train</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#create_sampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.create_sampler" title="Permalink to this definition">¶</a></dt>
<dd><p>This creates a sampler from the given weights</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weights</strong> (<em>either 0</em><em>, </em><em>1</em><em>, </em><em>dict</em><em> or </em><em>iterable</em>) – <p>if 0 (default) : no weights will be applied
if 1 : classification only, will balanced class with inverse frequency
if dict : keys are corresponding class values are sample weights
if iterable : list or np array must be of length equal to nb elements</p>
<blockquote>
<div><p>in the training set</p>
</div></blockquote>
</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Training targets</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.define_device">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">define_device</code><span class="sig-paren">(</span><em class="sig-param">device_name</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#define_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.define_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the device to use during training and inference.
If auto it will detect automatically whether to use cuda or cpu</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device_name</strong> (<em>str</em>) – Either “auto”, “cpu” or “cuda”</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Either “cpu” or “cuda”</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.filter_weights">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">filter_weights</code><span class="sig-paren">(</span><em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#filter_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.filter_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>This function makes sure that weights are in correct format for
regression and multitask TabNet</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weights</strong> (<em>int</em><em>, </em><em>dict</em><em> or </em><em>list</em>) – Initial weights parameters given by user</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>None</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>This function will only throw an error if format is wrong</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.utils.validate_eval_set">
<code class="sig-prename descclassname">pytorch_tabnet.utils.</code><code class="sig-name descname">validate_eval_set</code><span class="sig-paren">(</span><em class="sig-param">eval_set</em>, <em class="sig-param">eval_name</em>, <em class="sig-param">X_train</em>, <em class="sig-param">y_train</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/utils.html#validate_eval_set"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.utils.validate_eval_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if the shapes of eval_set are compatible with (X_train, y_train).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).
The last one is used for early stopping</p></li>
<li><p><strong>eval_name</strong> (<em>list of str</em>) – List of eval set names.</p></li>
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train owned products</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targeted products</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>eval_names</strong> (<em>list of str</em>) – Validated list of eval_names.</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – Validated list of eval_set.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pytorch_tabnet.multitask">
<span id="pytorch-tabnet-multitask-module"></span><h2>pytorch_tabnet.multitask module<a class="headerlink" href="#module-pytorch_tabnet.multitask" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier">
<em class="property">class </em><code class="sig-prename descclassname">pytorch_tabnet.multitask.</code><code class="sig-name descname">TabNetMultiTaskClassifier</code><span class="sig-paren">(</span><em class="sig-param">n_d: int = 8</em>, <em class="sig-param">n_a: int = 8</em>, <em class="sig-param">n_steps: int = 3</em>, <em class="sig-param">gamma: float = 1.3</em>, <em class="sig-param">cat_idxs: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_dims: List[int] = &lt;factory&gt;</em>, <em class="sig-param">cat_emb_dim: int = 1</em>, <em class="sig-param">n_independent: int = 2</em>, <em class="sig-param">n_shared: int = 2</em>, <em class="sig-param">epsilon: float = 1e-15</em>, <em class="sig-param">momentum: float = 0.02</em>, <em class="sig-param">lambda_sparse: float = 0.001</em>, <em class="sig-param">seed: int = 0</em>, <em class="sig-param">clip_value: int = 1</em>, <em class="sig-param">verbose: int = 1</em>, <em class="sig-param">optimizer_fn: Any = &lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">scheduler_fn: Any = None</em>, <em class="sig-param">scheduler_params: Dict = &lt;factory&gt;</em>, <em class="sig-param">mask_type: str = 'sparsemax'</em>, <em class="sig-param">input_dim: int = None</em>, <em class="sig-param">output_dim: int = None</em>, <em class="sig-param">device_name: str = 'auto'</em>, <em class="sig-param">n_shared_decoder: int = 1</em>, <em class="sig-param">n_indep_decoder: int = 1</em>, <em class="sig-param">grouped_features: List[List[int]] = &lt;factory&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytorch_tabnet.abstract_model.TabModel" title="pytorch_tabnet.abstract_model.TabModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_tabnet.abstract_model.TabModel</span></code></a></p>
<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.cat_dims">
<code class="sig-name descname">cat_dims</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.cat_dims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.cat_idxs">
<code class="sig-name descname">cat_idxs</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.cat_idxs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">y_true</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the loss according to network output and targets</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_pred</strong> (<em>list of tensors</em>) – Output of network</p></li>
<li><p><strong>y_true</strong> (<em>LongTensor</em>) – Targets label encoded</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – output of loss function(s)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.grouped_features">
<code class="sig-name descname">grouped_features</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.grouped_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.optimizer_params">
<code class="sig-name descname">optimizer_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite> or matrix: <cite>scipy.sparse.csr_matrix</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>results</strong> – Predictions of the most probable class</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions for classification on a batch (valid)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (a :tensor: <cite>torch.Tensor</cite> or matrix: <cite>scipy.sparse.csr_matrix</cite>) – Input data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>res</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.prepare_target">
<code class="sig-name descname">prepare_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.prepare_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.prepare_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare target before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (a :tensor: <cite>torch.Tensor</cite>) – Target matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Converted target matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.scheduler_params">
<code class="sig-name descname">scheduler_params</code><em class="property"> = None</em><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.stack_batches">
<code class="sig-name descname">stack_batches</code><span class="sig-paren">(</span><em class="sig-param">list_y_true</em>, <em class="sig-param">list_y_score</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.stack_batches"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.stack_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pytorch_tabnet.multitask.TabNetMultiTaskClassifier.update_fit_params">
<code class="sig-name descname">update_fit_params</code><span class="sig-paren">(</span><em class="sig-param">X_train</em>, <em class="sig-param">y_train</em>, <em class="sig-param">eval_set</em>, <em class="sig-param">weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multitask.html#TabNetMultiTaskClassifier.update_fit_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multitask.TabNetMultiTaskClassifier.update_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set attributes relative to fit function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>np.ndarray</em>) – Train set</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Train targets</p></li>
<li><p><strong>eval_set</strong> (<em>list of tuple</em>) – List of eval tuple set (X, y).</p></li>
<li><p><strong>weights</strong> (<em>bool</em><em> or </em><em>dictionnary</em>) – 0 for no balancing
1 for automated balancing</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-pytorch_tabnet.multiclass_utils">
<span id="pytorch-tabnet-multiclass-utils-module"></span><h2>pytorch_tabnet.multiclass_utils module<a class="headerlink" href="#module-pytorch_tabnet.multiclass_utils" title="Permalink to this headline">¶</a></h2>
<section id="multi-class-multi-label-utility-function">
<h3>Multi-class / multi-label utility function<a class="headerlink" href="#multi-class-multi-label-utility-function" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.assert_all_finite">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">assert_all_finite</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">allow_nan=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#assert_all_finite"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.assert_all_finite" title="Permalink to this definition">¶</a></dt>
<dd><p>Throw a ValueError if X contains NaN or infinity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array</em><em> or </em><em>sparse matrix</em>) – </p></li>
<li><p><strong>allow_nan</strong> (<em>bool</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.check_classification_targets">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">check_classification_targets</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#check_classification_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.check_classification_targets" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure that target y is of a non-regression type.</p>
<dl class="simple">
<dt>Only the following target types (as defined in type_of_target) are allowed:</dt><dd><p>‘binary’, ‘multiclass’, ‘multiclass-multioutput’,
‘multilabel-indicator’, ‘multilabel-sequences’</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (<em>array-like</em>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.check_output_dim">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">check_output_dim</code><span class="sig-paren">(</span><em class="sig-param">labels</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#check_output_dim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.check_output_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.check_unique_type">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">check_unique_type</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#check_unique_type"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.check_unique_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.infer_multitask_output">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">infer_multitask_output</code><span class="sig-paren">(</span><em class="sig-param">y_train</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#infer_multitask_output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.infer_multitask_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Infer output_dim from targets
This is for multiple tasks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y_train</strong> (<em>np.ndarray</em>) – Training targets</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>tasks_dims</strong> (<em>list</em>) – Number of classes for output</p></li>
<li><p><strong>tasks_labels</strong> (<em>list</em>) – List of sorted list of initial classes</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.infer_output_dim">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">infer_output_dim</code><span class="sig-paren">(</span><em class="sig-param">y_train</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#infer_output_dim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.infer_output_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Infer output_dim from targets</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y_train</strong> (<em>np.array</em>) – Training targets</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>output_dim</strong> (<em>int</em>) – Number of classes for output</p></li>
<li><p><strong>train_labels</strong> (<em>list</em>) – Sorted list of initial classes</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.is_multilabel">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">is_multilabel</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#is_multilabel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.is_multilabel" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if <code class="docutils literal notranslate"><span class="pre">y</span></code> is in a multilabel format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>]</em>) – Target values.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – Return <code class="docutils literal notranslate"><span class="pre">True</span></code>, if <code class="docutils literal notranslate"><span class="pre">y</span></code> is in a multilabel format, else <code class="docutils literal notranslate"><span class="pre">`False</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.utils.multiclass</span> <span class="kn">import</span> <span class="n">is_multilabel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[]])</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]))</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_multilabel</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.type_of_target">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">type_of_target</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#type_of_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.type_of_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Determine the type of data indicated by the target.</p>
<p>Note that this type is the most specific type that can be inferred.
For example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">binary</span></code> is more specific but compatible with <code class="docutils literal notranslate"><span class="pre">multiclass</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">multiclass</span></code> of integers is more specific but compatible with
<code class="docutils literal notranslate"><span class="pre">continuous</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">multilabel-indicator</span></code> is more specific but compatible with
<code class="docutils literal notranslate"><span class="pre">multiclass-multioutput</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> (<em>array-like</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>target_type</strong> – One of:</p>
<ul class="simple">
<li><p>’continuous’: <cite>y</cite> is an array-like of floats that are not all
integers, and is 1d or a column vector.</p></li>
<li><p>’continuous-multioutput’: <cite>y</cite> is a 2d array of floats that are
not all integers, and both dimensions are of size &gt; 1.</p></li>
<li><p>’binary’: <cite>y</cite> contains &lt;= 2 discrete values and is 1d or a column
vector.</p></li>
<li><p>’multiclass’: <cite>y</cite> contains more than two discrete values, is not a
sequence of sequences, and is 1d or a column vector.</p></li>
<li><p>’multiclass-multioutput’: <cite>y</cite> is a 2d array that contains more
than two discrete values, is not a sequence of sequences, and both
dimensions are of size &gt; 1.</p></li>
<li><p>’multilabel-indicator’: <cite>y</cite> is a label indicator matrix, an array
of two dimensions with at least two columns, and at most 2 unique
values.</p></li>
<li><p>’unknown’: <cite>y</cite> is array-like but none of the above, such as a 3d
array, sequence of sequences, or an array of non-sequence objects.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">])</span>
<span class="go">&#39;continuous&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">&#39;binary&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">])</span>
<span class="go">&#39;binary&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="go">&#39;binary&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">&#39;multiclass&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="go">&#39;multiclass&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="go">&#39;multiclass&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]))</span>
<span class="go">&#39;multiclass-multioutput&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="go">&#39;multiclass-multioutput&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">]]))</span>
<span class="go">&#39;continuous-multioutput&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_of_target</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]))</span>
<span class="go">&#39;multilabel-indicator&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pytorch_tabnet.multiclass_utils.unique_labels">
<code class="sig-prename descclassname">pytorch_tabnet.multiclass_utils.</code><code class="sig-name descname">unique_labels</code><span class="sig-paren">(</span><em class="sig-param">*ys</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_tabnet/multiclass_utils.html#unique_labels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytorch_tabnet.multiclass_utils.unique_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract an ordered array of unique labels</p>
<dl class="simple">
<dt>We don’t allow:</dt><dd><ul class="simple">
<li><p>mix of multilabel and multiclass (single label) targets</p></li>
<li><p>mix of label indicator matrix and anything else,
because there are no explicit labels)</p></li>
<li><p>mix of label indicator matrices of different sizes</p></li>
<li><p>mix of string and integer labels</p></li>
</ul>
</dd>
</dl>
<p>At the moment, we also don’t allow “multiclass-multioutput” input type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*ys</strong> (<em>array-likes</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – An ordered array of unique labels.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape [n_unique_labels]</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.utils.multiclass</span> <span class="kn">import</span> <span class="n">unique_labels</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unique_labels</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="go">array([3, 5, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unique_labels</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="go">array([1, 2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unique_labels</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>
<span class="go">array([ 1,  2,  5, 10, 11])</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="README.html" class="btn btn-neutral float-left" title="README" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019, Dreamquark

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>